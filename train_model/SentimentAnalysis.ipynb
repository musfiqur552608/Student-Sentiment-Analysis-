{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.2-cp310-cp310-win_amd64.whl (7.2 MB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\musfi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\musfi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (1.21.4)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\musfi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Using legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
            "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
            "    Running setup.py install for sklearn: started\n",
            "    Running setup.py install for sklearn: finished with status 'done'\n",
            "Successfully installed scikit-learn-1.0.2 sklearn-0.0 threadpoolctl-3.0.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zCfPcnweaQH2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0aQ86KWnad9v"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Tweets.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cVNyEfMEpqWs"
      },
      "outputs": [],
      "source": [
        "def normalizer(tweet):\n",
        "      only_letters = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
        "      only_letters = only_letters.lower()\n",
        "      only_letters = only_letters.split()\n",
        "      filtered_result = [word for word in only_letters if word not in stopwords.words('english')]\n",
        "      lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
        "      lemmas = ' '.join(lemmas)\n",
        "      return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\musfi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.6.5)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\musfi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2021.11.10)\n",
            "Requirement already satisfied: click in c:\\users\\musfi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.0.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\musfi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: joblib in c:\\users\\musfi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\musfi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4OlybFrgakHn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\musfi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "df = shuffle(df)\n",
        "y = df['airline_sentiment']\n",
        "x = df.text.apply(normalizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "67S4ac0vcmsd"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "x_vectorized = vectorizer.fit_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XG-2v96OejQk"
      },
      "outputs": [],
      "source": [
        "train_x,val_x,train_y,val_y = train_test_split(x_vectorized,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kwvcVXp5eua1"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "sparse multilabel-indicator for y is not supported.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12928/1148798647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# model = regressor.fit(train_x, train_y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrmfr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'entropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrmfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m         X, y = self._validate_data(\n\u001b[0;32m    328\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: sparse multilabel-indicator for y is not supported."
          ]
        }
      ],
      "source": [
        "regressor = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
        "model = regressor.fit(train_x, train_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IOa3q3kwadJb"
      },
      "outputs": [],
      "source": [
        "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
        "gs_clf = GridSearchCV(model, params, n_jobs=1, cv=5)\n",
        "gs_clf = gs_clf.fit(train_x, train_y)\n",
        "model = gs_clf.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YbPOoMF9eZcv"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(val_x)\n",
        "\n",
        "_f1 = f1_score(val_y, y_pred, average='micro')\n",
        "_confusion = confusion_matrix(val_y, y_pred)\n",
        "__precision = precision_score(val_y, y_pred, average='micro')\n",
        "_recall = recall_score(val_y, y_pred, average='micro')\n",
        "_statistics = {'f1_score': _f1,\n",
        "               'confusion_matrix': _confusion,\n",
        "               'precision': __precision,\n",
        "               'recall': _recall\n",
        "              }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "colab_type": "code",
        "id": "7p4_LYEkeqnU",
        "outputId": "3e746a79-b9c5-4e17-eb40-5ec0bcfd0056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['negative' 'negative' 'positive' ... 'negative' 'positive' 'negative']\n",
            "{'f1_score': 0.7811475409836065, 'confusion_matrix': array([[2013,  181,   67],\n",
            "       [ 275,  457,   74],\n",
            "       [ 130,   74,  389]], dtype=int64), 'precision': 0.7811475409836065, 'recall': 0.7811475409836065}\n"
          ]
        }
      ],
      "source": [
        "print(y_pred)\n",
        "print(_statistics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "_qASV9QMfJbE",
        "outputId": "e13fc9c7-7768-4758-83f8-6eb90735a7d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['negative'], dtype=object)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_feature = vectorizer.transform(['Meat Week Day 3: Tummy hurts every night'])\n",
        "model.predict(test_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "4SOrIgInoqhg",
        "outputId": "86cb3d51-95b5-41fe-e0f5-a143a186ac90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['positive'], dtype=object)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_feature = vectorizer.transform(['Movie is good'])\n",
        "model.predict(test_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "uWdDxP4Lxs37",
        "outputId": "5396ea9a-b698-403a-87a9-509b8698c029"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['neutral'], dtype=object)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_feature = vectorizer.transform(['I\\'m okay'])\n",
        "model.predict(test_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XZHJbO7qqymW"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TJsGuKgErxWX"
      },
      "outputs": [],
      "source": [
        "pickl = {'vectorizer': vectorizer,\n",
        "         'model': model\n",
        "         }\n",
        "pickle.dump(pickl, open('models'+\".p\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Wviph7W-m9zO"
      },
      "outputs": [],
      "source": [
        "with open('models.p', 'rb') as pickled:\n",
        "    data = pickle.load(pickled)\n",
        "\n",
        "model = data['model']\n",
        "vectorizer = data['vectorizer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"I am a bad boy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector = vectorizer.transform([text])\n",
        "prediction = model.predict(vector)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'negative'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SentimentAnalysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
